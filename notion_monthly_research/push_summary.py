import os
import sys
import time
import json
import requests
import feedparser
from datetime import datetime, timedelta
from urllib.parse import urlencode
from typing import List
from openai import OpenAI

# =========================
# ç¯å¢ƒå˜é‡ï¼ˆè¯·å…ˆåœ¨ç³»ç»Ÿé‡Œé…ç½®ï¼‰
# =========================
# DeepSeek
#   OPENAI_API_KEY=<ä½ çš„ DeepSeek API Key>
#   OPENAI_BASE_URL=https://api.deepseek.com   ï¼ˆä¹Ÿå¯ä¸è®¾ï¼Œç”¨ä¸‹é¢çš„é»˜è®¤ï¼‰
# Notion
#   NOTION_TOKEN=<ä½ çš„ Notion é›†æˆå¯†é’¥>
#   PAGE_ID=<è¦å†™å…¥çš„é¡µé¢æˆ–å—çš„ block_id>
# arXiv å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
#   QUERY_KEYWORDS="medical imaging, segmentation, ultrasound, CT"

DEEPSEEK_API_KEY = os.environ.get("OPENAI_API_KEY")
DEEPSEEK_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.deepseek.com")
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
PAGE_ID = os.environ.get("PAGE_ID")
QUERY_KEYWORDS = os.environ.get("QUERY_KEYWORDS","Large model fine-tuning, multimodal decoupling, multimodal alignment, multimodal enhancement")

if not DEEPSEEK_API_KEY:
    print("ERROR: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY ä¸ºä½ çš„ DeepSeek API Key")
    sys.exit(1)
if not NOTION_TOKEN or not PAGE_ID:
    print("ERROR: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ NOTION_TOKEN ä¸ PAGE_ID")
    sys.exit(1)

# åˆå§‹åŒ– DeepSeekï¼ˆOpenAI å…¼å®¹ï¼‰å®¢æˆ·ç«¯
client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)


# æ—¶é—´
today = datetime.today()
today_fmt = today.strftime("%Yå¹´%mæœˆ")
past_date = (today - timedelta(days=30)).strftime("%Y-%m-%d")


# =========================
# å·¥å…·å‡½æ•°
# =========================
def chunk_text(text: str, chunk_size: int = 1800) -> List[str]:
    """å°†é•¿æ–‡æœ¬åˆ‡æˆå¤šæ®µï¼Œé¿å… Notion å•å—è¿‡é•¿"""
    lines = text.splitlines()
    chunks, cur, cur_len = [], [], 0
    for ln in lines:
        ln_len = len(ln) + 1
        if cur_len + ln_len > chunk_size and cur:
            chunks.append("\n".join(cur))
            cur, cur_len = [], 0
        cur.append(ln)
        cur_len += ln_len
    if cur:
        chunks.append("\n".join(cur))
    return chunks


def notion_append_paragraph_blocks(page_or_block_id: str, markdown_text: str):
    """å°† Markdown æ–‡æœ¬æŒ‰æ®µè½å—è¿½åŠ åˆ° Notion æŒ‡å®šé¡µé¢/å—ä¸‹"""
    url = f"https://api.notion.com/v1/blocks/{page_or_block_id}/children"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }

    # ç®€å•æŒ‰æ®µè½æ‹†åˆ†ï¼ˆ\n\n ä½œä¸ºåˆ†æ®µï¼‰ï¼Œå¹¶è¿›ä¸€æ­¥åˆ†å—æ§åˆ¶æ¯æ®µé•¿åº¦
    paragraphs = markdown_text.split("\n\n")
    blocks = []
    for para in paragraphs:
        for piece in chunk_text(para, 1800):
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": piece}}]
                }
            })

    payload = {"children": blocks}
    # è½»é‡é‡è¯•
    for attempt in range(3):
        res = requests.patch(url, headers=headers, json=payload, timeout=30)
        if res.status_code in (200, 201):
            print("Notion push OK.")
            return
        print(f"Notion push failed [{res.status_code}]: {res.text}")
        if res.status_code in (429, 500, 502, 503, 504):
            time.sleep(2 ** attempt)
            continue
        break
    raise RuntimeError("Push to Notion failed.")


# =========================
# ä¸šåŠ¡é€»è¾‘
# =========================
def fetch_arxiv_papers(max_results=10) -> List[str]:
    """æŠ“å– arXiv æœ€è¿‘è®ºæ–‡ï¼ˆæŒ‰æäº¤æ—¶é—´å€’åºï¼‰ã€‚è¿”å› Markdown åˆ—è¡¨è¡Œã€‚"""
    base_url = "http://export.arxiv.org/api/query"
    keywords = [kw.strip() for kw in QUERY_KEYWORDS.split(",") if kw.strip()]
    raw_query = " OR ".join(keywords) if keywords else "medical imaging"
    params = {
        "search_query": f"all:{raw_query}",
        "start": 0,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }
    feed_url = f"{base_url}?{urlencode(params)}"
    feed = feedparser.parse(feed_url)

    papers = []
    for entry in feed.entries:
        title = entry.title.strip().replace("\n", " ")
        date = entry.published.split("T")[0] if hasattr(entry, "published") else ""
        link = entry.link
        papers.append(f"- [{date}] {title} ({link})")
    return papers


def generate_summary_from_papers(papers: List[str]) -> str:
    """ç”¨ DeepSeek ç”Ÿæˆ Markdown æ€»ç»“ï¼ˆOpenAI å…¼å®¹ Chat Completionsï¼‰"""
    papers_markdown = "\n".join(papers) if papers else "- ï¼ˆè¿‘30å¤©æœªæŠ“åˆ°ç›¸å…³è®ºæ–‡ï¼‰"
    prompt = f"""
ä»¥ä¸‹æ˜¯è¿‘ 30 å¤©å†…ä¸â€œ{QUERY_KEYWORDS}â€ç›¸å…³çš„ arXiv è®ºæ–‡åˆ—è¡¨ï¼Œè¯·æ ¹æ®å®ƒä»¬æ€»ç»“å½“å‰å¤šæ¨¡æ€è§£è€¦ã€èåˆã€å¯¹æ¯”ã€å¢å¼ºç ”ç©¶ä»¥åŠå¤§æ¨¡å‹å¾®è°ƒçš„å…³é”®è¶‹åŠ¿ã€çƒ­ç‚¹æ–¹å‘å’Œç ”ç©¶å…³æ³¨ç‚¹ã€‚è¾“å‡ºè¯·ä½¿ç”¨ Markdownï¼Œå¹¶æŒ‰ä»¥ä¸‹æ ¼å¼ï¼š

## {today_fmt} å¤šæ¨¡æ€ arXiv çƒ­ç‚¹è®ºæ–‡æ€»ç»“

### ğŸ” è¶‹åŠ¿æ¦‚è§ˆ
ï¼ˆç”±æ¨¡å‹ç”Ÿæˆçš„ç®€æ´è¦ç‚¹ï¼Œé¿å…ç©ºè¯å¥—è¯ï¼Œå°½é‡å¼•ç”¨è®ºæ–‡ä¸­çš„å¯éªŒè¯ä¿¡å·ï¼‰

### ğŸ“„ è®ºæ–‡åˆ—è¡¨
{papers_markdown}
""".strip()

    # è½»é‡é‡è¯•
    for attempt in range(3):
        try:
            resp = client.chat.completions.create(
                model="deepseek-reasoner",  # æˆ– "deepseek-reasoner"
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIç®—æ³•å’Œå¤§æ¨¡å‹ç ”ç©¶åˆ†æåŠ©æ‰‹"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5
            )
            return resp.choices[0].message.content
        except Exception as e:
            print(f"DeepSeek API è°ƒç”¨å¤±è´¥ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {e}")
            time.sleep(1.5 * (attempt + 1))
    raise RuntimeError("DeepSeek ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ/API Key/é¢åº¦ã€‚")


def main():
    print(f"Keywords: {QUERY_KEYWORDS}")
    print("Fetching arXiv...")
    papers = fetch_arxiv_papers(max_results=12)

    print("Generating summary with DeepSeek...")
    summary_md = generate_summary_from_papers(papers)

    print("Pushing to Notion...")
    notion_append_paragraph_blocks(PAGE_ID, summary_md)

    print("Done.")


if __name__ == "__main__":
    main()
